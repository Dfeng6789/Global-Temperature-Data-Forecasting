# -*- coding: utf-8 -*-
"""Final Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SVDJnT5vzw2jKsCqNV5Ek9qAYxDNvAPi

# CIS545 Final Project
## Ethan Chee, David Feng, and Lori Sun

The eventual goal is to use this dataset and others and train a machine learning model to see if it can predict the test temperatures we feed it and also possibly future temperatures, in light of global temperatures trends caused by global warming and other environmental components.

A video of our final project video, which was too large to submit to gradescope, can be found here: https://youtu.be/E-xfIh1e748

## Initial Setup and Imports
"""

!pip install pandas==1.1.5
# Import neccessary libraries
import pandas as pd
import numpy as np
import seaborn as sns
from string import ascii_letters
import matplotlib.pyplot as plt
import datetime as dt
import requests
from lxml import html

!nvidia-smi

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip3 install --upgrade mxnet-cu101 gluoncv

import mxnet as mx
from mxnet.gluon.data import DataLoader
from mxnet.gluon.data.vision import transforms
from mxnet import np

from mxnet import npx
from mxnet.gluon import nn
npx.set_np()

npx.num_gpus()

"""## Create and Clean Dataset on Global Land Temperatures

This dataset contains average land temperatures by city for every month spanning back all the way to the 1700s. Here we perform EDA and clean this data, removing much of the data from before the 1900s since we won't be able to join that data with the data from other datasets anyways, since they don't span as far back. 

In order to use the datasets in the project, just upload all the csv files uploaded to Gradescope to Colab under the Files tab using the Upload to session storage button. The only dataset that could not be uploaded due to its size is the main Global Land Temperatures By City csv because it was too large (it's the one right below this, and also takes a very long time to upload to Colab) but it is one of the recommended datasets and can be found at this link to Kaggle and directly downloaded from here.
https://www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data
"""

# Load the dataset and perform some basic EDA
temperatures_df = pd.read_csv('/content/GlobalLandTemperaturesByCity.csv')
temperatures_df

temperatures_df.info()

temperatures_df.head(10)

temperatures_df.describe()

# Drop all null values and remove data from before 1960 to reduce the size of the dataset for future cleaning and also because
# later when we merge with the other datasets, there isn't any data from before this point
temperatures_df = temperatures_df.dropna(axis=0, subset=["AverageTemperature", "City", "Country", "Latitude", "Longitude"])
temperatures_df = temperatures_df[temperatures_df["dt"] >= '1960-01-01'].reset_index(drop=True)
temperatures_df

# Convert longitude and latitude values into numerical floats, with South and West being converted into negative values
def new_lat(old_lat):
  latitude = ""
  if (old_lat[-1] == "N"):
    latitude = old_lat[:-1]
  elif (old_lat[-1] == "S"):
    latitude = "-" + old_lat[:-1]
  latitude = float(latitude)
  return latitude
temperatures_df["Latitude"] = temperatures_df["Latitude"].apply(lambda x: new_lat(x)) 

def new_long(old_long):
  longitude = old_long
  if (old_long[-1] == "E"):
    longitude = old_long[:-1]
  elif (old_long[-1] == "W"):
    longitude = "-" + old_long[:-1]
  longitude = float(longitude)
  return longitude
temperatures_df["Longitude"] = temperatures_df["Longitude"].apply(lambda x: new_long(x))
temperatures_df

temperatures_df.info()

# Converts datetime column into datetime type
temperatures_df['dt'] = temperatures_df['dt'].apply(lambda x : dt.datetime.strptime(x, '%Y-%m-%d'))
temperatures_df.info()

# Create separate year, day, and month columns
temperatures_df['Year'] = temperatures_df['dt'].apply(lambda x : x.year)
temperatures_df['Day'] = temperatures_df['dt'].apply(lambda x : x.day)
temperatures_df['Month'] = temperatures_df['dt'].apply(lambda x : x.month)
temperatures_df

# Find the average temperature worldwide for each datetime so that we can observe overall temperature trends over time
temperatures_by_date_df = temperatures_df.groupby(["dt"]).mean().reset_index()
temperatures_by_date_df

max_temps = temperatures_by_date_df.groupby(['Year']).max().reset_index()[['Year', 'AverageTemperature']]
max_temps = max_temps.rename(columns = {"AverageTemperature": "Maximum Temperature"})
min_temps = temperatures_by_date_df.groupby(['Year']).min().reset_index()[['Year', 'AverageTemperature']]
min_temps = min_temps.rename(columns = {"AverageTemperature": "Minimum Temperature"})
max_min_df = temperatures_by_date_df.merge(max_temps, how='inner', left_on=['Year'], right_on=['Year'])
max_min_df = max_min_df.merge(min_temps, how='inner', left_on=['Year'], right_on=['Year'])
max_min_df = max_min_df[["dt", "Maximum Temperature", "Minimum Temperature"]].set_index("dt")
plot = sns.lineplot(data=max_min_df)
plot.set(xlabel = "Date", ylabel = "Temperature", title="Maximum and Minimum Global Per Year Temperatures Over Time")
temperatures_by_date_df.plot(x = "dt", y = "AverageTemperature", xlabel="Date", ylabel="Average Temperature", title = "Average Global Temperature Over Time")
max_min_df = max_min_df.reset_index()
max_min_df.plot(x = "dt", y = "Maximum Temperature", xlabel="Date", ylabel="Maximum Temperature", title = "Maximum Global Temperature Per Year Over Time")
max_min_df.plot(x = "dt", y = "Minimum Temperature", xlabel="Date", ylabel="Minimum Temperature", title = "Minimum Global Temperature Per Year Over Time")

"""Clearly this plot of average temperatures over time fluctutates up and down each year due to seasonal changes. There seems to be minor increase in global average temperatures the last few years, but not as much as we would have expected based on narratives surrounding global warming."""

# Prepare the dataset for plotting by longitude and latitude to see its distribution across the globe
average_temperatures_no_date_df = temperatures_df.groupby(["City"]).mean().reset_index()
average_temperatures_no_date_df

plt.scatter(average_temperatures_no_date_df.Longitude, average_temperatures_no_date_df.Latitude, s=2, c=average_temperatures_no_date_df.AverageTemperature, cmap='plasma')
plt.title('Global Map of Average Temperatures')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.show()

temperatures_by_country_df = temperatures_df.groupby(['Country', "Year"]).mean().reset_index()
temperatures_by_country_df

import plotly.express as px

fig = px.scatter(temperatures_by_country_df, x = 'Longitude', y="Latitude", animation_frame="Year", animation_group="Country", color="AverageTemperature", hover_name="Country", size_max=20, width = 900, height = 490)

fig["layout"].pop("updatemenus") # optional, drop animation buttons
fig.show()

len(pd.unique(temperatures_df['City']))

"""We can see that our data is clustered in specific areas around the globe, which should be alright since our machine learning models will take location into account anyways when making predictions, and the data is still pretty widely spread across the entire world.

## Using Linear Regression to make predictions


Here we use linear regression on some of the features of our dataset to try and predict temperatures in the future based on time invariant data about latitude and longitude.
"""

# Stores numerical features dataframe into variable called "features"
features = temperatures_df[['Latitude', 'Longitude', 'Year', 'Month']]

# Maps a correlation matrix for the numerical features
corr = features.corr()
plt.figure(figsize=(5, 5))
plot = sns.heatmap(corr, center=0, square=True)
plot.set(title = 'Temperature Numerical Data Correlation Heatmap')

# Store the regression target variable into "temperatures"
temperatures = temperatures_df['AverageTemperature']

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, precision_score, recall_score, f1_score

# Normal Linear Regression
clf = LinearRegression()
clf.fit(features, temperatures)

#want to check how many unique locations we have data for
temperatures_df['LatLong'] = temperatures_df["Latitude"].astype(str) + " " + temperatures_df["Longitude"].astype(str) 
unique_locations = pd.unique(temperatures_df['LatLong'])
print(len(unique_locations))

future_df = pd.DataFrame(unique_locations, columns = ['LatLong'])
future_df = pd.merge(future_df, temperatures_df[['City', 'Country','Latitude', 'Longitude', 'LatLong']], on = 'LatLong', how = 'inner')
future_df.dtypes
temperatures_df.dtypes

future_df = future_df.groupby(['LatLong']).min()
future_df = future_df.reset_index()
future_df

future_df['dates'] = [pd.date_range(start='1-1-2022', periods=12, freq='M') for i in range(len(future_df['City']))]
future_df = future_df.explode('dates')
future_df = future_df.reset_index()
future_df['Year'] = future_df['dates'].apply(lambda x : x.year)
future_df['Month'] = future_df['dates'].apply(lambda x : x.month)
future_df

x_prediction = future_df[['Latitude', 'Longitude', 'Year', 'Month']]
y_pred = clf.predict(x_prediction)
future_df['predicted_temperature'] = y_pred
future_df

predictions_2022 = future_df.groupby('LatLong').mean()
predictions_2022
plt.scatter(predictions_2022.Longitude, predictions_2022.Latitude, s=2, c=predictions_2022.predicted_temperature, cmap='plasma')
plt.title('Global Map of 2022 Predictions')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.show()

from sklearn.linear_model import Ridge
clf = Ridge(alpha=1.0)
clf.fit(features, temperatures)
y_pred = clf.predict(x_prediction)
future_df['predicted_temperature'] = y_pred
predictions_2022 = future_df.groupby('LatLong').mean()
predictions_2022
plt.scatter(predictions_2022.Longitude, predictions_2022.Latitude, s=2, c=predictions_2022.predicted_temperature, cmap='plasma')
plt.title('Global Map of 2022 Predictions')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.show()

"""Both of our models created predictions that do not look very simliar to the initial map we plotted. Our predictions show that there is a positive correlation between being south and being hot. In the actual dataset, the correlation is only true for data above the equator.

What probably happened here is that we have many more data points above the equator and thus the trends the model follows are more similar to the patterns above the equator. Let's check to see if this is true.
"""

north_df = temperatures_df.loc[(temperatures_df['Latitude'] >= 0)]
print(len(north_df))
south_df = temperatures_df.loc[(temperatures_df['Latitude'] <= 0)]
print(len(south_df))

"""We can see that there is much more data in the northern hemisphere. Clearly there was an issue because the data was skewed towards trends in the nothern hemisphere. To try to mitigate this let's use distance from the equator instead of latitude."""

temperatures_df['Distance_From_Equator'] = temperatures_df['Latitude'].apply(lambda x: abs(x))
features = temperatures_df[['Distance_From_Equator', 'Longitude', 'Year', 'Month']]
temperatures = temperatures_df['AverageTemperature']
clf = LinearRegression()
clf.fit(features, temperatures)
future_df['Distance_From_Equator'] = future_df['Latitude'].apply(lambda x: abs(x))
x_prediction = future_df[['Distance_From_Equator', 'Longitude', 'Year', 'Month']]
y_pred = clf.predict(x_prediction)
future_df['predicted_temperature'] = y_pred
predictions_2022 = future_df.groupby('LatLong').mean()
plt.scatter(predictions_2022.Longitude, predictions_2022.Latitude, s=2, c=predictions_2022.predicted_temperature, cmap='plasma')
plt.title('Global Map of 2022 Predictions After Equator Adjustment')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.show()

"""This map looks much more accurate than the other two!

## Create and Clean Dataset on Environmental Technology Patents
https://data.oecd.org/envpolicy/patents-on-environment-technologies.htm

Encouraging collaboration on technology development is particularly pertinent when addressing global climate 
change or regional pollution. Importantly, international collaboration in research and technology can help 
local businesses take advantage of existing technologies (i.e. help build local absorptive capacity). This, in turn, helps increase the uptake of cleaner technologies globally.
"""

patent_df = pd.read_csv('/content/Enviro-tech_patent.csv')
patent_df = patent_df[["LOCATION", "TIME", "Value"]]
patent_df = patent_df.rename(columns = {"LOCATION": "Country", "TIME": "Year", "Value": "Patents"}).sort_values(by='Year')
patent_df

# Dataset of countries and their 3 letter country codes from https://www.kaggle.com/datasets/juanumusic/countries-iso-codes
country_code_df = pd.read_csv('/content/country_codes.csv')
country_code_df = country_code_df.rename(columns = {"English short name lower case" : "Name", "Alpha-3 code": "Code"})
country_code_df = country_code_df.reset_index()
# Merge the two datasets together so that we can convert country codes into country names for joining
patent_df = patent_df.merge(country_code_df, how='inner', left_on='Country', right_on='Code')
patent_df = patent_df[['Name', 'Patents', 'Year']]
patent_df = patent_df.rename(columns = {"Name" : "Country"}).sort_values(by='Patents', ascending=False).reset_index(drop = True)
patent_df

# Find the average number of patents per country and then plot the result
patent_by_country_df = patent_df.groupby(["Country"]).mean().reset_index()
patent_by_country_df = patent_by_country_df[["Country", "Patents"]]
patent_by_country_df = patent_by_country_df.sort_values(by='Patents', ascending=False)
patent_by_country_df

sns.set(rc={"figure.figsize":(20, 10)})
patentplot = sns.barplot(x = 'Country',y = 'Patents', data = patent_by_country_df)
patentplot.set(title = "Average Number of Patents Per Country")
patentplot.set_xlabel("Countries", fontsize = 12)
patentplot.set_ylabel("Number of Patents", fontsize = 12)
patentplot.set_xticklabels(patentplot.get_xticklabels(), rotation = 90, size = 6.6)

"""## Create and Clean Dataset on Environmental Policy
https://stats.oecd.org/Index.aspx?DataSetCode=EPS#

The OECD Environmental Policy Stringency Index (EPS) is a country-specific and internationally-comparable measure of the stringency of environmental policy. Stringency is defined as the degree to which environmental policies put an explicit or implicit price on polluting or environmentally harmful behaviour. The index ranges from 0 (not stringent) to 6 (highest degree of stringency). The index covers 28 OECD and 6 BRIICS countries for the period 1990-2012. The index is based on the degree of stringency of 14 environmental policy instruments, primarily related to climate and air pollution.
"""

policy_df = pd.read_csv('/content/enviro_policy.csv')
policy_df

# We keep the columns that are important: Variable = 'Environmental Policy Stringency', Country, Value 
policy_df.info()
policy_df = policy_df[["Country", "Variable", "Value", "Year"]]
policy_df = policy_df.loc[policy_df['Variable'] == "Environmental Policy Stringency"]
policy_df

policy_df = policy_df[["Country", "Value", "Year"]]
policy_df

# Here once again Value stands for Environmental Policy Stringency, we take the average per year and plot the result
policy_by_date_df = policy_df.groupby(["Year"]).mean().reset_index()
policy_by_date_df.plot(x = "Year", y = "Value", ylabel="Environmental Policy Stringency", title = "Environmental Policy Stringency Over Time")

"""Strictness of environmental policy increases over time globally with an interesting dip around 2014"""

# Find average stringency by country, and then plot the result
policy_by_country_df = policy_df.groupby(["Country"]).mean().reset_index()
policy_by_country_df = policy_by_country_df[["Country", "Value"]]
policy_by_country_df = policy_by_country_df.sort_values(by='Value', ascending=False)
policy_by_country_df

sns.set(rc={"figure.figsize":(10, 8)})
policyplot = sns.barplot(x = 'Country',y = 'Value', data = policy_by_country_df)
policyplot.set(title = "Average Policy Stringency Index Per Country")
policyplot.set_xlabel("Countries", fontsize = 12)
policyplot.set_ylabel("Policy Stringency Index", fontsize = 12)
policyplot.set_xticklabels(policyplot.get_xticklabels(), rotation = 90, size = 6.6)

"""The most strict countries are all European: Denmark, Germany, Switzerland, Austria. The least strict are China, India, South Africa, Indonesia, Russia, and Brazil.

## Merge the Datasets Together

Here we combine all of the datasets that we cleaned in the previous steps, merging on the country and the year. We also pull out numerical columns and the target average temperature column in preparation for the machine learning component to be performed below.
"""

# Merge in the environmental policy stringency and patent number datasets, by country and year
merged_df = temperatures_df.merge(policy_df, how='inner', left_on=['Country', 'Year'], right_on=['Country', 'Year'])
merged_df = merged_df.rename(columns = {"Value": "Stringency", "AverageTemperatureUncertainty": "Uncertainty"})
merged_df = merged_df.merge(patent_df, how='inner', left_on=['Country', 'Year'], right_on=['Country', 'Year'])
merged_df

# Turns the categorical country column into a numerical one by converting every country to a number based on its index in the country codes dataframe
merged_df = merged_df.merge(country_code_df[['index', 'Name']], how='inner', left_on=['Country'], right_on=['Name'])
merged_df = merged_df.rename(columns = {"index": "Country ID"})
merged_df

# Stores numerical features dataframe into variable called "features"
features = merged_df[['Uncertainty', 'Latitude', 'Longitude', 'Year', 'Month', 'Stringency', 'Patents', 'Country ID']]

# Maps a correlation matrix for the numerical features
corr = features.corr()
plt.figure(figsize=(8, 8))
plot = sns.heatmap(corr, center=0, square=True)
plot.set(title = 'Temperature Numerical Data Correlation Heatmap')

# Store the regression target variable into "temperatures"
temperatures = merged_df['AverageTemperature']

"""## Linear Regression in sklearn

Here we use linear regression on the merged dataset to train various linear regression models to try and find the one that performs the best on our dataset. 


"""

from sklearn.model_selection import train_test_split

# Conduct 80/20 train-test split with random_state = 42
x_train, x_test, y_train, y_test = train_test_split(features, temperatures, test_size = 0.2, random_state = 42)

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, precision_score, recall_score, f1_score

# Normal Linear Regression
clf = LinearRegression()
clf.fit(x_train, y_train)
y_pred = clf.predict(x_test)
score = r2_score(y_test, y_pred)
score

"""Ordinary linear regression has minimized the mean squared error, but might overfit or contain multicollinearity, and so we try Lasso, Ridge, and Elastic Net Regression to try and address this."""

from sklearn.linear_model import Lasso

# Lasso (L1) Regression
clf = Lasso(alpha=1.0)
clf.fit(x_train, y_train)
y_pred = clf.predict(x_test)
lasso_score = r2_score(y_test, y_pred)
lasso_score

from sklearn.linear_model import Ridge

# Ridge (L2) Regression
clf = Ridge(alpha=1.0)
clf.fit(x_train, y_train)
y_pred = clf.predict(x_test)
ridge_score = r2_score(y_test, y_pred)
ridge_score

from sklearn.linear_model import ElasticNet

# Elastic Net (L1 + L2) Regression
clf = ElasticNet(alpha=1.0)
clf.fit(x_train, y_train)
y_pred = clf.predict(x_test)
elastic_net_score = r2_score(y_test, y_pred)
elastic_net_score

"""We can also try to eliminate collinearity between variables by performing PCA before we train our model."""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
pca = PCA(n_components=8)
pca.fit_transform(StandardScaler().fit_transform(x_test))

explained_variance_ratios = pca.explained_variance_ratio_
cum_evr = pca.explained_variance_ratio_.cumsum()
cum_evr

# We plot the cumulative explained variance ratio to decide on the number of components to keep, in this case the minimum
# number of components that explain at least 80% of total variance in the dataset
plt.figure(figsize=(8, 5))
# We do this to shift the number of components up by 1, since plotting without doing this starts a 0 given array indexing
evr_df = pd.DataFrame(cum_evr).reset_index()
evr_df['index'] = evr_df['index'].apply(lambda x : x + 1)
evr_df
plot = sns.lineplot(data=evr_df, x = 'index', y = 0)
plot.set_xlabel("Number of Components", fontsize=10)
plot.set_ylabel("Cumulative Explained Variance Ratio", fontsize=10)
plot.set(title="Cumulative Explained Variance Ratio Based on the Number of Components")

from sklearn.pipeline import Pipeline

# Pipelined Linear Regression with PCA
pca2 = PCA(n_components=5)
pca2.fit_transform(StandardScaler().fit_transform(x_test))
scl = StandardScaler()
clf = LinearRegression()
pipe = Pipeline(steps=[('Scale', scl), ('PCA', pca2), ('LinReg', clf)])
pipe.fit(x_train, y_train)
prediction = pipe.predict(x_test)
test_accuracy = r2_score(y_test, prediction)
test_accuracy

# Linear Regression with PCA
pca3 = PCA(n_components=5)
clf2 = LinearRegression()
x_train_pca = pca3.fit_transform(StandardScaler().fit_transform(x_train))
clf2.fit(x_train_pca, y_train)
x_test_pca = pca3.transform(StandardScaler().fit_transform(x_test))
prediction2 = clf2.predict(x_test_pca)
test_accuracy2 = r2_score(y_test, prediction2)
test_accuracy2

score_bar_data = {"Regular": score, "Lasso": lasso_score, "Ridge": ridge_score, "Elastic Net": elastic_net_score, 
                  "Pipeline PCA": test_accuracy, "Regular PCA": test_accuracy2}
sns.set(rc={"figure.figsize":(10, 8)})
bar_series_data = pd.Series(score_bar_data, name="Accuracy Score").to_frame().reset_index()
bar_series_data
plot = sns.barplot(x="index", y="Accuracy Score", data= bar_series_data)
plot.set(xlabel = 'Linear Regression Type')
plot.set(ylabel = 'Accuracy Score')
plot.set(title = 'Accuracy Score for Each Type of Linear Regression')
plot.set(ylim=(0.3, 0.4))
plot.tick_params(axis='x', which='major')

"""Overall we can see that regular linear regression and ridge regression performed the best out of all the different methods used, and had very similar results. This is because it is likely that our list of features contains multicollinearity, given the relationships between country code and longitude and latitude, as well as between these specific countries and their policies and patent numbers. Regular linear regression tends to have the possibility of overfitting here, and Ridge regression can do better than Lasso in the presence of collinearity, explaining these high performance values.

We note that perorming PCA did not significantly change our testing accuracy, and in fact caused it to go down, which is likely because we do not have that many features, and also because our features are generally not correlated.

We do not perform predictions using this linear regression as we did above, because the data here like environmental stringency is not time invariant like the location data from above, and so we cannot fill in this data for the future to make predictions with it using linear regression. However, these models can be used to fill in gaps in temperature data from past dates where we do have this other numerical data.

## Deep Learning Using Neural Networks

Here we train a feedforward neural network model to try and gauge its performance on the dataset. As is mentioned in the challenges section, originally without normalizing the data this neural network's loss would fluctuate wildly between different values, sometimes ending up at higher values than it started at, likely because of the wide range of numerical values in the data. However, after normalizing, a clear downward trend can be noted.
"""

# This line is used to significantly speed up the training of the deep neural net, can be commented out or adjusted but generally
# if we use the whole dataset the training takes an incredibly long time
new_merged_df = merged_df[merged_df.dt >= '2011-01-01'].reset_index(drop = True)
new_merged_df.info()
# Stores numerical features dataframe into variable
deep_features = new_merged_df[['Uncertainty', 'Latitude', 'Longitude', 'Year', 'Month', 'Stringency', 'Patents', 'Country ID']]
# Store the regression target variable
deep_temperatures = new_merged_df['AverageTemperature']

x_train, x_test, y_train, y_test = train_test_split(deep_features, deep_temperatures, test_size = 0.2)
def NormalizeData(data):
    return (data - np.min(data)) / (np.max(data) - np.min(data))

x_train = NormalizeData(np.array(x_train.to_numpy(), dtype=np.float32))
y_train = NormalizeData(np.array(y_train.to_numpy(dtype=np.float32).reshape(-1,1), dtype=np.float32))
x_test = NormalizeData(np.array(x_test.to_numpy(), dtype=np.float32))
y_test = NormalizeData(np.array(y_test.to_numpy().reshape(-1,1), dtype=np.float32))
train_dataset = mx.gluon.data.dataset.ArrayDataset(x_train, y_train)
test_dataset = mx.gluon.data.dataset.ArrayDataset(x_test, y_test)
train_data = mx.gluon.data.DataLoader(train_dataset, 10, shuffle=True)
test_data = mx.gluon.data.DataLoader(test_dataset, 1, shuffle=False)

from mxnet.gluon import nn
from mxnet import gluon, autograd, ndarray
from mxnet import init

def construct_net():
    net = nn.Sequential()
    with net.name_scope():
      net.add(nn.Dense(10))
      net.add(nn.Dense(1))
    return net

net = construct_net()

ctx =  mx.gpu() if mx.context.num_gpus() else mx.cpu()

net.initialize(init.Normal(sigma=0.01), ctx=ctx)

"""Here we use L1 Loss, also called Mean Absolute Error, which computes the sum of absolute distance between target values and the output of the neural network.

Further described here: https://mxnet.apache.org/versions/1.7/api/python/docs/tutorials/packages/gluon/loss/loss.html
"""

from mxnet.gluon import loss as gloss

metric = mx.metric.Accuracy()

loss = gloss.L2Loss()

trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': 0.001})
loss_plot_dict = {}
for epoch in range(10):
    total_accuracy = 0
    for x, y in train_data:
      x = x.as_in_context(ctx)
      y = y.as_in_context(ctx)
      with autograd.record():
          l = loss(net(x), y)
      l.backward()
      trainer.step(10)
      metric.update(preds = net(x).as_nd_ndarray(), labels = y)
      total_accuracy += metric.get()[1].item()
    x_train = x_train.as_in_context(ctx)
    y_train = y_train.as_in_context(ctx)
    metric.reset()
    epoch_loss = loss(net(x_train), y_train).mean().item()
    epoch_accuracy = total_accuracy / len(train_data)
    loss_plot_dict[epoch] = epoch_loss
    print(f'\nEpoch {epoch} Loss: {epoch_loss}')
    print(f'\nEpoch {epoch} Accuracy: {epoch_accuracy}')

loss_plot_df = pd.DataFrame.from_dict(loss_plot_dict, orient='index')
plt.plot(loss_plot_df)
plt.title("Change in Loss Across Epochs")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.show()

total_accuracy = 0
for x, y in test_data:
  x = x.as_in_context(ctx)
  y = y.as_in_context(ctx)
  with autograd.record():
      l = loss(net(x), y)
  l.backward()
  metric.update(preds = net(x).as_nd_ndarray(), labels = y)
  total_accuracy += metric.get()[1].item()
x_train = x_train.as_in_context(ctx)
y_train = y_train.as_in_context(ctx)
metric.reset()
epoch_loss = loss(net(x_train), y_train).mean().item()
epoch_accuracy = total_accuracy / len(test_data)
print(f'\nTest Loss: {epoch_loss}')
print(f'\nTest Accuracy: {epoch_accuracy}')

"""Note that the loss and accuracy outputs fluctuate somewhat, likely because of the small subset of the dataset we are using to reduce the training set, although there is still clearly a downward trend in loss (the value is small because before we run the neural network we normalize all data to be between 0 and 1). If we were to include all of the data that we have and train over more epochs, it is likely that the model would perform better.

## New Jersey Data
In our previous dataset we did not have many columns which probably made the models less accurate. In this section we will try the same models on a dataset with more features that are weather related to see if that improves accuracy.

The data below was pulled from NOAA's global summary of the month dataset. https://www.ncdc.noaa.gov/cdo-web/datasets

In order to get the data used below I queried the data set for weather data from New Jersey one year at a time because there was a limit on how much data you could order at once. Then I combined 10 years worth of data into one dataset.
"""

NJ_df = pd.read_csv('/content/NJ_weather.csv')
NJ_df

"""Looking at the dataframe there seems to be a lot of data missing. If we were to get rid of all rows with at least one NaN we would not have enough data points. Let's use a threshold instead. However since we are predicting on average temperature we need to make sure we get rid of all NaN's in the TAVG column."""

NJ_df = NJ_df.dropna(thresh=8).dropna(subset=['TAVG'])
NJ_df = NJ_df.drop(columns=['AWND'])
NJ_df['Year'] = NJ_df['DATE'].apply(lambda x : int(x[:4]))
NJ_df['Month'] = NJ_df['DATE'].apply(lambda x : int(x[5:]))
NJ_df = NJ_df.fillna(0)
NJ_df = NJ_df.rename(columns = {'CLDD': 'COOLING_DEGREE_DAYS', 'HTDD': 'HEATING_DEGREE_DAYS', 'PRCP': 'PRECIPITATION'})
NJ_df

plt.scatter(NJ_df.LONGITUDE, NJ_df.LATITUDE, s=2, c=NJ_df.TAVG, cmap='plasma')
plt.title('NJ average Temperatures')
plt.xlabel('longitude')
plt.ylabel('latitude')
plt.show()

# Stores numerical features dataframe into variable called "features"
features = NJ_df[['LATITUDE', 'LONGITUDE', 'ELEVATION', 'Year', 'Month', 'COOLING_DEGREE_DAYS', 'HEATING_DEGREE_DAYS', 'PRECIPITATION']]

# Maps a correlation matrix for the numerical features and the target variable.
corr = NJ_df[['LATITUDE', 'LONGITUDE', 'ELEVATION', 'Year', 'Month', 'COOLING_DEGREE_DAYS', 'HEATING_DEGREE_DAYS', 'PRECIPITATION', 'TAVG']].corr()
plt.figure(figsize=(9, 9))
plot = sns.heatmap(corr, center=0, square=True)
plot.set(title = 'Temperature Numerical Data Correlation Heatmap')

# Store the regression target variable into "temperatures"
temperatures = NJ_df['TAVG']

"""Now we will perform Linear Regression"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, precision_score, recall_score, f1_score
from sklearn.linear_model import Lasso
from sklearn.linear_model import Ridge
from sklearn.linear_model import ElasticNet

# Conduct 80/20 train-test split with random_state = 42
x_train, x_test, y_train, y_test = train_test_split(features, temperatures, test_size = 0.2, random_state = 42)

# Normal Linear Regression
clf = LinearRegression()
clf.fit(x_train, y_train)
y_pred = clf.predict(x_test)
score = r2_score(y_test, y_pred)
score

# Lasso (L1) Regression
clf = Lasso(alpha=1.0)
clf.fit(x_train, y_train)
y_pred = clf.predict(x_test)
lasso_score = r2_score(y_test, y_pred)
lasso_score

# Ridge (L2) Regression
clf = Ridge(alpha=1.0)
clf.fit(x_train, y_train)
y_pred = clf.predict(x_test)
ridge_score = r2_score(y_test, y_pred)
ridge_score

# Elastic Net (L1 + L2) Regression
clf = ElasticNet(alpha=1.0)
clf.fit(x_train, y_train)
y_pred = clf.predict(x_test)
elastic_net_score = r2_score(y_test, y_pred)
elastic_net_score

"""The R^2 scores of all of the linear regression models were all very high. Next we will use a feed forward NN."""

deep_features = NJ_df[['LATITUDE', 'LONGITUDE', 'ELEVATION', 'Year', 'Month', 'COOLING_DEGREE_DAYS', 'HEATING_DEGREE_DAYS', 'PRECIPITATION']]
# Store the regression target variable
deep_temperatures = NJ_df['TAVG']

x_train, x_test, y_train, y_test = train_test_split(deep_features, deep_temperatures, test_size = 0.2)
x_train = np.array(x_train.to_numpy(), dtype=np.float32)
y_train = np.array(y_train.to_numpy(dtype=np.float32).reshape(-1,1), dtype=np.float32)
x_test = np.array(x_test.to_numpy(), dtype=np.float32)
y_test = np.array(y_test.to_numpy().reshape(-1,1), dtype=np.float32)
train_dataset = mx.gluon.data.dataset.ArrayDataset(x_train, y_train)
test_dataset = mx.gluon.data.dataset.ArrayDataset(x_test, y_test)
train_data = mx.gluon.data.DataLoader(train_dataset, 10, shuffle=True)
test_data = mx.gluon.data.DataLoader(test_dataset, 1, shuffle=False)

from mxnet.gluon import nn
from mxnet import gluon, autograd, ndarray
from mxnet import init

def construct_net():
    net = nn.Sequential()
    with net.name_scope():
      net.add(nn.Dense(5))
      net.add(nn.Dense(1))
    return net

net = construct_net()

ctx =  mx.gpu() if mx.context.num_gpus() else mx.cpu()

net.initialize(init.Normal(sigma=0.01), ctx=ctx)

from sklearn import metrics
from mxnet.gluon import loss as gloss

def train_network(net, train_loader, criterion, trainer, metric, epochs):
    accuracies = []
    losses = []
    epoch_list = [i for i in range(epochs)]
    for i in range(epochs):
      metric.reset()
      total_accuracy = 0
      total_loss = 0
      samples = 0
      for data, labels in train_loader:
        data = data.reshape((data.shape[0], -1))
        data = data.as_in_context(ctx)
        labels = labels.as_in_context(ctx)
        with autograd.record():
          output = net(data)
          loss = criterion(output,labels)
        loss.backward()
        trainer.step(data.shape[0])
        metric.update(labels=labels.as_nd_ndarray(), preds = output.as_nd_ndarray())
        total_loss += loss.sum()
        samples += data.shape[0]
        total_accuracy = metric.get()[1]
      losses.append(total_loss/samples)
      accuracies.append(metric.get()[1])
    print('accuracy:' + str(accuracies) + ' loss:' + str(losses))
    plt.plot(epoch_list, accuracies, label = "accuracy")
    plt.title("epochs vs accuracy")
    plt.xlabel("epoch")
    plt.ylabel("accuracy")
    plt.show()
    plt.plot(epoch_list, losses, label = "loss")
    plt.title("epochs vs loss")
    plt.xlabel("epoch")
    plt.ylabel('loss')
    plt.show()
    final_training_accuracy = float(metric.get()[1])
    final_training_loss = float(total_loss/samples)
    return final_training_accuracy, final_training_loss
  
trainer = gluon.Trainer(net.collect_params(), 'adam', optimizer_params={'learning_rate': 0.001})
epochs = 80
metric = mx.metric.Accuracy()
from mxnet.gluon import loss as gloss
loss = gloss.L2Loss()
lr_training_accuracy, lr_training_loss = train_network(net, train_data, loss, trainer, metric, epochs)
print(lr_training_accuracy, lr_training_loss)

def test_model(net, criterion, test_loader, metric):
    total_accuracy = 0
    total_loss = 0
    samples = 0
    predictions = []
    actual = []
    metric.reset()
    for data, labels in test_loader:
      data = data.reshape((data.shape[0], -1))
      data = data.as_in_context(ctx)
      labels = labels.as_in_context(ctx)
      with autograd.record():
        output = net(data)
        loss = criterion(output,labels)
      predictions.append(output.item())
      actual.append(labels.item())
      metric.update(labels=labels.as_nd_ndarray(), preds = output.as_nd_ndarray())
      total_loss += loss.sum()
      samples += data.shape[0]
      total_accuracy = metric.get()[1]
    print('loss: ' + str(total_loss/samples) + ' accuracy: ' + str(total_accuracy))
    tests = [i for i in range(len(actual))]
    plt.plot(tests[:100], actual[:100], label = "actual")
    plt.plot(tests[:100], predictions[:100], label = "predictions", alpha = 0.4)
    plt.legend()
    plt.title("Actual Temperatures vs Predicted Temperatures (truncated to 100 data points)")
    plt.xlabel('Trial Number')
    plt.ylabel('Temperature')
    plt.show()
    testing_accuracy = float(total_accuracy)
    testing_loss = float(total_loss/samples)
    return  testing_loss, testing_accuracy
metric = mx.metric.Accuracy()  
lr_testing_loss, lr_testing_accuracy = test_model(net, loss, test_data, metric)

NJ_2011_df = pd.read_csv('/content/NJ_weather_2011.csv')
NJ_2011_df = NJ_2011_df.dropna(thresh=8).dropna(subset=['TAVG'])
NJ_2011_df = NJ_2011_df.drop(columns=['AWND'])
NJ_2011_df['Year'] = NJ_2011_df['DATE'].apply(lambda x : int(x[:4]))
NJ_2011_df['Month'] = NJ_2011_df['DATE'].apply(lambda x : int(x[5:]))
NJ_2011_df = NJ_2011_df.fillna(0)
NJ_2011_df = NJ_2011_df.rename(columns = {'CLDD': 'COOLING_DEGREE_DAYS', 'HTDD': 'HEATING_DEGREE_DAYS', 'PRCP': 'PRECIPITATION'})
NJ_2011_df

deep_features_2011 = NJ_2011_df[['LATITUDE', 'LONGITUDE', 'ELEVATION', 'Year', 'Month', 'COOLING_DEGREE_DAYS', 'HEATING_DEGREE_DAYS', 'PRECIPITATION']]
# Store the regression target variable
deep_temperatures_2011 = NJ_2011_df['TAVG']
deep_features_2011 = np.array(deep_features_2011.to_numpy(), dtype=np.float32)
deep_temperatures_2011 = np.array(deep_temperatures_2011.to_numpy().reshape(-1,1), dtype=np.float32)
test_2011_dataset = mx.gluon.data.dataset.ArrayDataset(deep_features_2011, deep_temperatures_2011)
test_2011_data = mx.gluon.data.DataLoader(test_2011_dataset, 1, shuffle=False)
metric = mx.metric.Accuracy()  
lr_testing_loss, lr_testing_accuracy = test_model(net, loss, test_2011_data, metric)

"""For the above graph, the gray line is where the actual value and the predicted value overlap.

While the accuracy is not that high in the graph, the loss is relatively low. For the purposes of predicting temperature a 3.2 loss is pretty low. Thus, the model is pretty successful. 

While this model can't be used to make predictions on future temperatures because we don't have access to climate data for the future, it can be used to fill in missing historical data. For example, I had to eliminate a lot of NaN's in the NJ dataset but with our model above we could try to estimate what those NaN's would have been.

## Description of Challenges/ Obstacles Faced



1.   Setting up the neural network was quite difficult, as it despite trying out all manners of different losses, layers, initializations, trainer parameters, and batch sizes, for the longest time the model would just fluctuate back and forth between large loss values, sometimes ending on a higher loss than it started at. We eventually fixed this problem by normalizing all the data to be between the range 0 and 1.

2.   Finding extra data to join was also an obstacle initially, since our temperature dataset doesn't in and of itself contain many features that could be used to train linear regression/neural network models. Finding data that had information on date and location, as well as a useful numerical component useful for predicting temperature, was definitely a challenge although we eventually merged in outside data to create more features for machine learning.

3. Predicting future temperatures was very difficult because most of the features used to predict temperatures in our models, like climate and policy scores, are data that we do not have for the future. For example, we can't use the rainfall from 2023 to predict temperatures in 2023 because we don't know what the rainfall in 2023 will be. As a result, our model used to predict future temperatures was only trained off of data that does not change overtime, which in the case of our dataset was just location.

## Potential Next Steps/ Future Direction


1.   We use linear regression to predict future values, but we don't do so with the neural network. Given more time, we could also use the neural network that we trained to predict future temperature values.

2.  Despite merging in various datasets into our original temperature dataset, there is still definitely room to find other features that might predict future temperature changes, and add them to the dataset to better train the machine learning models. Things like spending on environmental research, reliance on fossil fuels, and general weather forecasts are all things that could be researched, cleaned, and merged into the dataset to add more features to the machine learning model.

3. ARIMA can be used to predict/forecast data values that are seasonal, which our dataset of temperatures clearly is. Given more time ARIMA could definitely be used to try and predict the same temperatures, and its performance could be compared to that of our linear regression to see which performs better.
"""